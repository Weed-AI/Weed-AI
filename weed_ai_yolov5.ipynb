{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajH__57r-aKt"
      },
      "source": [
        "# Train YOLOv5 (and v8 soon!) on Weed-AI Datasets\n",
        "\n",
        "This guide will take you through training a state-of-the-art object detection architecture - YOLOv5 - on Weed-AI datasets.\n",
        "\n",
        "**Steps:**\n",
        "1. Setup the project: creating folders, cloning YOLOv5\n",
        "2. Download the Weed-AI dataset\n",
        "3. Convert weedCOCO to YOLO annotation format\n",
        "4. Create YOLOv5 supporting files\n",
        "5. Train YOLOv5\n",
        "6. Inference on pictures/videos\n",
        "\n",
        "The tutorial requires you to have access to a Google Drive account and be able to upload images/data to specific folders. Algorithms will train fastest with a GPU. Select the GPU type under 'Runtime' > 'Change Runtime Type'. Make sure GPU is selected. Premium or High RAM will improve speed/size of models that can be trained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qMUXWJhJwiI"
      },
      "outputs": [],
      "source": [
        "# mount google drive - this gives the Colab notebook access to your Drive. It may ask you for permission/to sign in too.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ5sO03S4QNc"
      },
      "source": [
        "# Create Project Folder\n",
        "\n",
        "To begin, create a project folder in your Google Drive. We'll call this one `weedai_yolo`. Replace this with whatever name you decide. \n",
        "\n",
        "It will be created in the root folder of your Google Drive. In this folder we'll be cloning the [YOLOv5 GitHub Repository](https://github.com/ultralytics/yolov5) and saving our data too. There are many guides on training YOLOv5 that are accessible through the official repository, make sure to check those for any tips/tricks on tuning your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fv8VN0rKEew"
      },
      "outputs": [],
      "source": [
        "YOUR_DIRECTORY = 'weedai_yolo'\n",
        "\n",
        "!mkdir /content/drive/MyDrive/{YOUR_DIRECTORY}\n",
        "%ls '/content/drive/MyDrive/' # should list everything in your Google Drive - double check that your project folder is there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaKbq-Iv5BAi"
      },
      "source": [
        "**(first time only)**\n",
        "\n",
        "Clone the YOLOv5 repository so we can use it to train our models. Only do this ONCE at the start of the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqBQG78rW-o4"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/{YOUR_DIRECTORY}\n",
        "!git clone https://github.com/ultralytics/yolov5 # clone the YOLOv5 repository. It is a large repository and may take some time depending on your internet speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwTXpIk7yN6T"
      },
      "source": [
        "# Downloading a Weed-AI dataset\n",
        "\n",
        "For this example I've used the [Northern WA Wheatbelt Blue Lupins](https://weed-ai.sydney.edu.au/datasets/9df290f4-a29b-44b2-9de6-24bca1cee846) dataset but any of the other object detection datasets would work too, including the recrntly uploaded [Amsinckia in chickpeas](https://weed-ai.sydney.edu.au/datasets/21675efe-9d25-4096-be76-3a541475efd4) dataset. \n",
        "\n",
        "Download the dataset. Then, we'll create a datasets folder in the YOLOv5 directory and then move the Weed-AI dataset to that folder. \n",
        "\n",
        "To summarise, the steps we will follow below are:\n",
        "1. Download the dataset on Weed-AI by clicking the button 'Download in WEEDCOCO format'\n",
        "2. Unzip the download and move it into the Google Drive Yolov5 folder where your project has been created. For me, this is the `'weedai_yolo/yolov5/datasets'`\n",
        "3. Convert it from WeedCOCO to YOLOv5 format\n",
        "\n",
        "Assuming you've downloaded the dataset and unzipped it, follow the steps below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oMTfghKpOrW"
      },
      "outputs": [],
      "source": [
        "YOUR_DATASET = 'blue_lupins' # set the name of your dataset folder here. It should match the name of the folder with images and the weedcoco.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv16-Y3gzDmV"
      },
      "source": [
        "\n",
        "\n",
        "Create the dataset folder where you'll move the unzipped folder renamed to `blue_lupins` to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or9RrDKE5grb"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/drive/MyDrive/{YOUR_DIRECTORY}/yolov5/datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsZVQqLu7ubB"
      },
      "source": [
        "Once the dataset has downloaded and is in the datasets folder, it should have a similar structure to the following:\n",
        "* yolov5/datasets\n",
        "    * blue_lupins\n",
        "        * images\n",
        "        * weedcoco.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBzTasNQx3Va"
      },
      "source": [
        "# Convert weedCOCO to YOLO\n",
        "\n",
        "The first step in the process is converting the downloaded weedCOCO dataset into the YOLO .txt format. The method below is adapted from the official [Ultralytics GitHub repository](https://github.com/ultralytics/JSON2YOLO/blob/master/labelbox_json2yolo.py). Don't worry too much about the code, just press 'Play' on each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJT7joLkwvN-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import yaml\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import contextlib\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "\n",
        "def make_dirs(dir='new_dir/'):\n",
        "    # Create folders\n",
        "    dir = Path(dir)\n",
        "    for p in dir, dir / 'labels', dir / 'images':\n",
        "        p.mkdir(parents=True, exist_ok=True)  # make dir\n",
        "    return dir\n",
        "\n",
        "\n",
        "def convert_weedcoco_json(json_dir=''):\n",
        "    save_dir = make_dirs(dir=f'{json_dir}')  # output directory\n",
        "    print()\n",
        "\n",
        "    # Import json\n",
        "    for json_file in sorted(Path(json_dir).resolve().glob('*.json')):\n",
        "        fn = Path(save_dir) # / 'labels' # folder name\n",
        "        fn.mkdir(exist_ok=True)\n",
        "        with open(json_file) as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Create image dict\n",
        "        images = {'%g' % x['id']: x for x in data['images']}\n",
        "        # Create image-annotations dict\n",
        "        imgToAnns = defaultdict(list)\n",
        "        for ann in data['annotations']:\n",
        "            imgToAnns[ann['image_id']].append(ann)\n",
        " \n",
        "\n",
        "        # Write labels file\n",
        "        for img_id, anns in tqdm(imgToAnns.items(), desc=f'Annotations {json_file}'):\n",
        "            # print(img_id, anns)\n",
        "            img = images['%g' % img_id]\n",
        "            h, w, f = img['height'], img['width'], img['file_name']\n",
        "\n",
        "            bboxes = []\n",
        "            segments = []\n",
        "            for ann in anns:\n",
        "                # The COCO box format is [top left x, top left y, width, height]\n",
        "                box = np.array(ann['bbox'], dtype=np.float64)\n",
        "                box[:2] += box[2:] / 2  # xy top-left corner to center\n",
        "                box[[0, 2]] /= w  # normalize x\n",
        "                box[[1, 3]] /= h  # normalize y\n",
        "                if box[2] <= 0 or box[3] <= 0:  # if w <= 0 and h <= 0\n",
        "                    continue\n",
        "\n",
        "                cls = ann['category_id']  # class\n",
        "                box = [cls] + box.tolist()\n",
        "                if box not in bboxes:\n",
        "                    bboxes.append(box)\n",
        "\n",
        "            # Write\n",
        "            with open((fn / f.replace('images', 'labels')).with_suffix('.txt'), 'a') as file:\n",
        "                for i in range(len(bboxes)):\n",
        "                    line = *(bboxes[i]),  # cls, box or segments\n",
        "                    file.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "    # Save dataset.yaml\n",
        "    names = [data['categories'][i]['name'].split(': ')[1] for i in range(len(data['categories']))]\n",
        "    d = {'path': json_dir,\n",
        "         'train': 'images/train',\n",
        "         'val': 'images/train',\n",
        "         'test': 'images/train',\n",
        "         'nc': len(names),\n",
        "         'names': names}  # dictionary\n",
        "\n",
        "    with open(f\"{save_dir}/weedcoco.yaml\", 'w') as f:\n",
        "        yaml.dump(d, f, sort_keys=False)\n",
        "\n",
        "\n",
        "    print('\\nConversion completed successfully!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8rarUs3JGEr"
      },
      "outputs": [],
      "source": [
        "WEED_COCO_LOCATION = f\"/content/drive/MyDrive/{YOUR_DIRECTORY}/yolov5/datasets/{YOUR_DATASET}\"\n",
        "#convert the weedcoco file\n",
        "convert_weedcoco_json(json_dir=WEED_COCO_LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgX1Xd81Z-F4"
      },
      "source": [
        "## Splitting the dataset into train/validation/test\n",
        "An algorithm needs a training portion and a validation portion to check as it learns. The test portion is left entirely unseen and can be used later to make sure the algorithm hasn't just overfit. \n",
        "\n",
        "If you find the algorithm performs well on the training data but terribly on the val/test data, then it is likely overfitting. This is more common on small datasets and larger models when trained for many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrqzLzamZ93e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read images and annotations\n",
        "images = [os.path.join(f'{WEED_COCO_LOCATION}/images', x) for x in os.listdir(f'{WEED_COCO_LOCATION}/images')]\n",
        "annotations = [os.path.join(f'{WEED_COCO_LOCATION}/labels', x) for x in os.listdir(f'{WEED_COCO_LOCATION}/labels') if x[-3:] == \"txt\"]\n",
        "\n",
        "images.sort()\n",
        "annotations.sort()\n",
        "\n",
        "# Split the dataset into train-valid-test splits 80-10-10%\n",
        "train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 1)\n",
        "val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 1)\n",
        "\n",
        "%cd {WEED_COCO_LOCATION}\n",
        "!mkdir images/train images/val images/test labels/train labels/val labels/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxpec1tmcyKq"
      },
      "outputs": [],
      "source": [
        "#Utility function to move images \n",
        "def move_files_to_folder(list_of_files, destination_folder):\n",
        "    for f in list_of_files:\n",
        "        try:\n",
        "            shutil.move(f, destination_folder)\n",
        "        except:\n",
        "            print(f)\n",
        "            assert False\n",
        "\n",
        "# Move the splits into their folders\n",
        "move_files_to_folder(train_images, 'images/train')\n",
        "move_files_to_folder(val_images, 'images/val/')\n",
        "move_files_to_folder(test_images, 'images/test/')\n",
        "move_files_to_folder(train_annotations, 'labels/train/')\n",
        "move_files_to_folder(val_annotations, 'labels/val/')\n",
        "move_files_to_folder(test_annotations, 'labels/test/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slC0GAIMc7TC"
      },
      "outputs": [],
      "source": [
        "# Check the images have been moved\n",
        "print(len(os.listdir('images/train')), len(os.listdir('labels/train')))\n",
        "print(len(os.listdir('images/val')), len(os.listdir('labels/val')))\n",
        "print(len(os.listdir('images/test')), len(os.listdir('labels/test')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i0acGTl0TNN"
      },
      "source": [
        "# Preparing for training\n",
        "Now we have all the splits made, we need to import some packages and install other YOLOv5 requirements before we can start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IiPCt6JFb_"
      },
      "outputs": [],
      "source": [
        "# import necessary packages\n",
        "import torch\n",
        "from IPython.display import Image  # for displaying images\n",
        "import os \n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "print('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2e4sDVtI_I3"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/{YOUR_DIRECTORY}/yolov5\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgPBKBGiFsLS"
      },
      "outputs": [],
      "source": [
        "# Weights & Biases  (optional)\n",
        "%pip install -q wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlS2qYfF5bt8"
      },
      "source": [
        "# YOLOv5 Training\n",
        "\n",
        "No we can train a model! Change the name of your run to whatever you like, and try playing around with things like image size, batch size, epochs and YOLOv5 variant. Larger variants and larger images will probably do better, but require more memory. So if you run out of memory, just reduce image size or model variant size (choose M instead of X) and then try again.\n",
        "\n",
        "Information on selecting batch size: https://twitter.com/rasbt/status/1617544195220312066\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yeIX7cSDhfIJ"
      },
      "outputs": [],
      "source": [
        "# train YOLOv5m\n",
        "BATCH = 8\n",
        "EPOCHS = 30\n",
        "IMAGE_SIZE = 1280 # (should be one of 320, 640, 1280, 1920)\n",
        "MODEL = 'm'\n",
        "\n",
        "# this is the name of your run, and how it will be saved\n",
        "RUN_NAME = f'{YOUR_DATASET}_TRAIN_B{str(BATCH)}_E{str(EPOCHS)}_SZ{str(IMAGE_SIZE)}_M{MODEL}'\n",
        "\n",
        "# avoid making any changes to the below, or check the Ultralytics docs for other commands\n",
        "!python train.py --img {IMAGE_SIZE} --cfg yolov5{MODEL}.yaml --batch {BATCH} --epochs {EPOCHS} --data datasets/{YOUR_DATASET}/weedcoco.yaml --weights yolov5{MODEL}.pt --name {RUN_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY9BG5FG2chA"
      },
      "source": [
        "# Detect\n",
        "This is where you can run the model you've just trained on a sample video or other dataset to see how it goes. The --source flag below accepts videos, folders of images and images. All you need to do is upload these to the YOLOv5 datasets directory and then specify the name/path below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_k1PVJpwL_f"
      },
      "outputs": [],
      "source": [
        "DETECTION_FILES = '' # e.g. 'test_video.mp4' OR test_image_directory OR test_image.jpg\n",
        "CONFIDENCE_THRESHOLD = 0.50 # this should be between 0 and 1. It changes the cutoff value for a detection. Lower = more sensitive, higher = less sensitive\n",
        "\n",
        "!python detect.py --source datasets/{DETECTION_FILES} --weights runs/train/{RUN_NAME}/weights/best.pt --name {RUN_NAME} --img {IMAGE_SIZE} --conf-thres 0.50"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}